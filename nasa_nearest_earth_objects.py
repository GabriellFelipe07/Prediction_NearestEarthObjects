# -*- coding: utf-8 -*-
"""NASA - Nearest Earth Objects

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XXjTGCs0QAK-bhpkIDSq8WzkH7DuLddg

# **NASA - Nearest Earth Objects**

Autor: Gabriel Felipe Machado de Oliveira, 2022

# 1. Introdução
Este projeto é um estudo de Ciência de Dados considerando uma base dados tiradas do NASA Open API e NEO Earth Close Approaches. Através dela, a ideia é aplicar métodos de tratamento de dados e técnicas de Machine Learning para a predição da variável resposta.

## 1.1. Dataset: NASA - Nearest Earth Objects
O dataset utilizado fornecem valorese características de objetos que orbitam o planeta terra possui. Há 10 colunas, são elas:
1. id: Identificador pra cada asteróide;
2. name: Um nome dado pela NASA ao objeto;
3. est_diameter_min: Valor mínimo estimado do diâmetro do objeto;
4. est_diameter_max: Valor máximo estimado do diâmetro do objeto;
5. relative_velocity: Velocidade relativa da terra no momento de captação do objeto;
6. miss_distance: Distância em Km do objeto;
7. orbiting_body: Corpo na qual o objeto está orbitando;
8. sentry_object: Se houve ou não registro de colisão do asteróide;
9. absolute_magnitude: Descrição da luminosidade do objeto;
10. hazardous: Se o objeto fornece perigo a terra ou não;

A partir disso, o objetivo é realizar a predição da variável hazardous, considerando todas os outros atributos.

# 2. Importação dos Dados

Para importar e visualizar o nosso dataset precisaremos de algumas bibliotecas.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv('neo.csv')
data

"""Analisando se há dados faltantes e os tipos de dados."""

data.info()

"""# 3. Tratamento e Visualização dos Dados

Primeiro, vamos analisar cada um dos atributos quantitativos através de histogramas de frequência.
"""

data.hist(bins = 20, figsize = (10,10), color = 'lightgreen') 
plt.show()

"""Pensando em nossa predição, faremos algumas alterações no dataset. Perceba que os atributos id e name são irrelevantes para o nosso objetivo final: predição da variável resposta, uma vez que são atributos categóricos e estamos interessados em variáveis numéricas. Assim, excluiremos as duas colunas."""

data = data.drop(columns = ['id', 'name'])
data.info()

"""Perceba ainda o comportamento da variável orbiting_body e da variável sentry_object."""

data['orbiting_body'].unique()

data['sentry_object'].unique()

"""Ambas, além de serem categóricas possuem apenas um valor em todas as linhas. Dessa forma, também excluiremos do nosso dataset."""

data = data.drop(columns = ['orbiting_body', 'sentry_object'])
data.info()

data.shape

"""Agora, possuímos um dataset com 90836 linhas e 6 colunas.

# 4. Análise Exploratória e Estatística Descritiva

Agora, faremos um análise mais profunda da nossa base. Isto é, focaremos na análise estatística e em suas aplicações para os nossos modelos de predição. Inicialmente, observemos algumas medidas:
"""

data.describe()

"""Com isso, seria interessante verificarmos as correlações entre os atributos."""

data.corr()

"""Graficamente, as correlações são:"""

plt.figure(figsize = (19,8))
sns.heatmap(data.corr(), annot = True)
plt.show()

"""Observe que temos uma alta correlação entre duas variáveis explicativas: est_diameter_max e est_diameter_min. Para o nosso futuro modelo isso não é interessante, uma vez que certamente atrapalhará o rendimento das nossas predições. Nesse caso, seria melhor excluirmos do dataset um desses dois atributos. 
Aqui, excluiremos o est_diameter_min.
"""

data = data.drop(columns = ['est_diameter_min'])

data.shape

"""Agora, temos uma base de dados com 5 atributos e 90836 linhas."""

plt.figure(figsize = (19,8))
sns.heatmap(data.corr(), annot = True)
plt.show()

"""Analisando novamente os histogramas de frequência e boxplots:"""

data.hist(bins = 100, figsize = (10,10), color = 'lightgreen') 
plt.show()

data.boxplot(figsize = (10,10))
plt.show()

"""Vamos agora, analisar a variável resposta."""

data['hazardous'].value_counts()

"""Para melhor predizê-lá, converteremos ela pra valores inteiros, True = 1 False = 0."""

data['num_hazardous'] = data['hazardous'].map( {True: 1, False: 0} ).astype(int)
display(data)

"""E excluíremos a coluna antiga."""

data = data.drop(columns = ['hazardous'])
data['num_hazardous'].value_counts()

"""Finalmente, temos o seguinte dataset:"""

data

"""# 5. Modelos de Classificação, Preparação e Predição dos dados

Importaremos alguma bibliotecas necessárias para modelarmos nossos dados e fazermos as predições.
"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier
from sklearn.tree import DecisionTreeClassifier

"""## 5.1. Separação dados de treino e teste

Primeiro, separaremos o dataset entre variáveis explicativas e variável reposta.
"""

X = data.drop(columns = ['num_hazardous'])
y = data['num_hazardous']

print(X.shape, y.shape)

"""Agora, separaremos os dados de treinamento e teste, sendo os dados de teste 20% das linhas totais do nosso dataset."""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

accuracyLista = []

"""## 5.2. Logistic Regression"""

logistic_regression = LogisticRegression()
logistic_regression.fit(X_train, y_train)
y_pred = logistic_regression.predict(X_test)

accuracyLR = round(accuracy_score(y_test, y_pred), 4)
accuracyLista.append(accuracyLR)
print('Accuracy: ', accuracyLR)

"""## 5.3. XGBClassifier"""

xgbclassifier = XGBClassifier()
xgbclassifier.fit(X_train, y_train)
y_pred = xgbclassifier.predict(X_test)

accuracyxgb = round(accuracy_score(y_test, y_pred),4)
accuracyLista.append(accuracyxgb)
print('Accuracy:', accuracyxgb)

"""## 5.4. KNeighborsClassifier """

kclassifier = KNeighborsClassifier()
kclassifier.fit(X_train, y_train)
y_pred = kclassifier.predict(X_test)

accuracyknn = round(accuracy_score(y_test, y_pred), 4)
accuracyLista.append(accuracyknn)
print('Accuracy:', accuracyknn)

"""## 5.5. RandomForestClassifier"""

randomforest = RandomForestClassifier()
randomforest.fit(X_train, y_train)
y_pred = randomforest.predict(X_test)

accuracyrandom = round(accuracy_score(y_test, y_pred), 4) 
accuracyLista.append(accuracyrandom)
print('Accuracy:', accuracyrandom)

"""## 5.6. GaussianNB"""

gaussian = GaussianNB()
gaussian.fit(X_train, y_train)
y_pred = gaussian.predict(X_test)

accuracygaussian = round(accuracy_score(y_test, y_pred), 4)
accuracyLista.append(accuracygaussian)
print('Accuracy:', accuracygaussian)

"""## 5.7. SGDClassifier"""

sgdc = SGDClassifier()
sgdc.fit(X_train, y_train)
y_pred = sgdc.predict(X_test)

accuracysgdc = round(accuracy_score(y_test, y_pred), 4)
accuracyLista.append(accuracysgdc)
print('Accuracy:', accuracysgdc)

"""## 5.7. DecisionTreeClassifier"""

dectree = DecisionTreeClassifier()
dectree.fit(X_train, y_train)
y_pred = dectree.predict(X_test)

accuracydectree = round(accuracy_score(y_test, y_pred), 4)
accuracyLista.append(accuracydectree)
print('Accuracy:', accuracydectree)

"""## 5.8 Sumarização das Acurácias

Facilitando nossa visualização das acurácias, faremos um DataFrame dos resultados de cada modelo.
"""

classificadores = ['Logistic Regression', 'XGBClassifier', 'KNeighborsClassifier', 'RandomForestClassifier', 'GaussianNB', 'SGDClassifier', 'DecisionTreeClassifier']

dic = {'Models': classificadores, 'Accuracy': accuracyLista}
sum = pd.DataFrame(dic)
sum

"""# 6. Conclusão

Observe abaixo a acurácia de cada modelo na qual treinamos e testamos o nosso dataset
"""

sum.sort_values(by='Accuracy', ascending=False)

"""O três melhores modelos não-paramétricos para a predição da variável resposta são: Ramdom Forest, XGBoost e o Logistic Regression. Os três conseguiram acurácias maiores que 90%. 

Assim, concluímos que para a predição e classificação da prejudicialidade de corpos que orbitam a terra a melhor opção seria utilizar a classifiação Random Forest.
"""